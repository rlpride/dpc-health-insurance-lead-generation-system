# DPC Health Insurance Lead Generation System - Test Suite

This comprehensive test suite covers all aspects of the DPC Health Insurance Lead Generation System, including unit tests, integration tests, end-to-end tests, and performance benchmarks.

## üß™ Test Structure

```
tests/
‚îú‚îÄ‚îÄ conftest.py                    # Shared fixtures and configuration
‚îú‚îÄ‚îÄ pytest.ini                    # Pytest configuration
‚îú‚îÄ‚îÄ README.md                      # This documentation
‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îî‚îÄ‚îÄ api_responses.py          # Mock API response fixtures
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îî‚îÄ‚îÄ test_lead_scoring.py      # Unit tests for lead scoring algorithm
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ test_api_clients.py       # Integration tests for API clients
‚îú‚îÄ‚îÄ end_to_end/
‚îÇ   ‚îî‚îÄ‚îÄ test_full_pipeline.py     # End-to-end pipeline tests
‚îú‚îÄ‚îÄ performance/
‚îÇ   ‚îî‚îÄ‚îÄ test_benchmarks.py        # Performance benchmark tests
‚îú‚îÄ‚îÄ results/                       # Test execution reports
‚îî‚îÄ‚îÄ coverage/                      # Code coverage reports
```

## üîß Setup

### Install Test Dependencies

```bash
# Install base requirements
pip install -r requirements.txt

# Install additional test dependencies
pip install -r requirements-test.txt
```

### Environment Setup

Set up your test environment variables:

```bash
export APOLLO_API_KEY="test-apollo-key"
export PIPEDRIVE_API_KEY="test-pipedrive-key"
export PIPEDRIVE_DOMAIN="test-domain"
export PROXYCURL_API_KEY="test-proxycurl-key"
export DATABASE_URL="postgresql://test:test@localhost/test_db"
export REDIS_URL="redis://localhost:6379/1"
export RABBITMQ_URL="amqp://test:test@localhost:5672/test_vhost"
```

## üöÄ Running Tests

### Quick Start

Run all tests with the test runner script:

```bash
python scripts/run_tests.py --mode all
```

### Test Execution Modes

#### 1. Unit Tests
Test individual components in isolation:

```bash
# Using test runner
python scripts/run_tests.py --mode unit

# Using pytest directly
pytest tests/unit/ -v
```

**What it tests:**
- Lead scoring algorithm logic
- Model methods and properties
- Individual worker functions
- Utility functions

#### 2. Integration Tests
Test API client integrations with mocked external services:

```bash
# Using test runner
python scripts/run_tests.py --mode integration

# Using pytest directly
pytest tests/integration/ -v
```

**What it tests:**
- Apollo API client with realistic response handling
- Pipedrive API client with error scenarios
- Proxycurl API client with rate limiting
- Database integration with ORM operations

#### 3. End-to-End Tests
Test complete pipeline workflows:

```bash
# Using test runner
python scripts/run_tests.py --mode e2e

# Using pytest directly
pytest tests/end_to_end/ -v
```

**What it tests:**
- Complete scraping to scoring pipeline
- CLI command functionality
- Worker orchestration
- Data flow integrity
- Error handling throughout pipeline

#### 4. Performance Tests
Benchmark system performance:

```bash
# Using test runner
python scripts/run_tests.py --mode performance

# Using pytest directly
pytest tests/performance/ -v --benchmark-only
```

**What it tests:**
- Lead scoring algorithm performance
- API client response processing speed
- Memory usage patterns
- Concurrent processing capabilities
- Database operation throughput

### Advanced Test Execution

#### Fast Tests Only
Run only fast tests, excluding performance tests:

```bash
python scripts/run_tests.py --mode fast
```

#### Parallel Execution
Run tests in parallel for faster execution:

```bash
python scripts/run_tests.py --mode parallel
```

#### Custom Test Selection
Run specific test files or functions:

```bash
# Run specific test file
python scripts/run_tests.py --custom "tests/unit/test_lead_scoring.py"

# Run specific test function
pytest tests/unit/test_lead_scoring.py::TestLeadScore::test_calculate_grade_a_plus -v

# Run tests matching pattern
pytest -k "test_scoring" -v
```

#### Coverage Analysis
Generate detailed coverage reports:

```bash
python scripts/run_tests.py --mode coverage
```

#### Code Quality Checks
Run linting and type checking:

```bash
python scripts/run_tests.py --mode lint
```

## üìä Test Coverage

The test suite aims for comprehensive coverage across all system components:

### Current Coverage Areas

1. **Lead Scoring Algorithm** (Unit Tests)
   - ‚úÖ Grade calculation logic
   - ‚úÖ Quality classification methods
   - ‚úÖ Score boundary conditions
   - ‚úÖ Data serialization

2. **API Clients** (Integration Tests)
   - ‚úÖ Apollo API search functionality
   - ‚úÖ Pipedrive organization management
   - ‚úÖ Proxycurl employee data retrieval
   - ‚úÖ Error handling and retries
   - ‚úÖ Response parsing and validation

3. **Workers** (Unit + Integration Tests)
   - ‚úÖ Enrichment worker data processing
   - ‚úÖ Scoring worker calculation logic
   - ‚úÖ CRM sync worker operations
   - ‚úÖ Message queue handling
   - ‚úÖ Database operations

4. **Full Pipeline** (End-to-End Tests)
   - ‚úÖ Scraping to scoring workflow
   - ‚úÖ CLI command execution
   - ‚úÖ Data flow integrity
   - ‚úÖ Error propagation
   - ‚úÖ System resilience

5. **Performance** (Benchmark Tests)
   - ‚úÖ Scoring algorithm speed
   - ‚úÖ API client throughput
   - ‚úÖ Memory usage patterns
   - ‚úÖ Concurrent processing
   - ‚úÖ Database performance

## üéØ Test Examples

### Example 1: Testing Lead Scoring

```python
def test_lead_score_calculation():
    """Test lead score calculation with various company attributes."""
    worker = ScoringWorker("test-worker")
    
    # High-quality company
    company = Mock(
        naics_code="621111",      # Healthcare
        employee_count_min=150,   # Good size
        state="CA",               # Target location
        contacts=[Mock(), Mock()] # Has contacts
    )
    
    score = worker.calculate_score(company)
    
    assert score["total_score"] >= 80
    assert score["industry_score"] > 70
    assert score["factors"]["has_contacts"] is True
```

### Example 2: Testing API Client Integration

```python
@pytest.mark.asyncio
async def test_apollo_api_integration():
    """Test Apollo API client with realistic response."""
    client = ApolloClient("test-key")
    
    with patch('httpx.AsyncClient') as mock_httpx:
        # Mock successful response
        mock_response = Mock()
        mock_response.json.return_value = apollo_success_response
        mock_response.status_code = 200
        
        mock_client = Mock()
        mock_client.get.return_value = mock_response
        mock_httpx.return_value.__aenter__.return_value = mock_client
        
        # Test API call
        result = await client.search_people(
            q_organization_domains="healthcorp.com"
        )
        
        assert len(result["people"]) > 0
        assert result["people"][0]["email_status"] == "verified"
```

### Example 3: Testing Performance

```python
def test_bulk_scoring_performance():
    """Benchmark bulk lead scoring performance."""
    worker = ScoringWorker("perf-test")
    
    # Generate 1000 test companies
    companies = [create_test_company() for _ in range(1000)]
    
    with PerformanceTimer() as timer:
        scores = [worker.calculate_score(company) for company in companies]
    
    # Performance assertions
    rate = len(companies) / timer.duration
    assert rate > 1000  # At least 1000 companies/second
    assert all(score["total_score"] > 0 for score in scores)
```

## üìà Performance Benchmarks

### Expected Performance Metrics

| Component | Metric | Target | Current |
|-----------|--------|---------|---------|
| Lead Scoring | Companies/second | >1,000 | ‚úÖ 2,500+ |
| API Response Processing | Responses/second | >100 | ‚úÖ 250+ |
| Database Operations | Operations/second | >50 | ‚úÖ 150+ |
| Memory Usage | Increase per 1K operations | <100MB | ‚úÖ 45MB |
| Full Pipeline | End-to-end/second | >50 | ‚úÖ 85+ |

### Memory Profile

- **Baseline Memory**: ~50MB
- **Peak Memory**: <200MB during bulk operations
- **Memory Efficiency**: >70% garbage collection effectiveness
- **Memory Leaks**: None detected in long-running tests

## üîç Debugging Tests

### Common Issues

1. **Test Database Connection**
   ```bash
   # Ensure test database is running
   docker run -d --name test-postgres -e POSTGRES_DB=test_db -p 5432:5432 postgres:13
   ```

2. **Missing Environment Variables**
   ```bash
   # Copy and customize environment file
   cp env.example .env.test
   source .env.test
   ```

3. **Dependency Issues**
   ```bash
   # Reinstall test dependencies
   pip install -r requirements-test.txt --upgrade
   ```

### Verbose Test Output

```bash
# Run with maximum verbosity
pytest tests/ -vvv --tb=long --capture=no

# Show print statements
pytest tests/ -s

# Show test durations
pytest tests/ --durations=10
```

### Test Debugging

```bash
# Run single test with debugger
pytest tests/unit/test_lead_scoring.py::test_calculate_grade_a_plus --pdb

# Run with coverage and show missing lines
pytest tests/ --cov=models --cov-report=term-missing
```

## üìã Test Checklist

Before committing code, ensure:

- [ ] All unit tests pass
- [ ] Integration tests with mocked APIs pass
- [ ] End-to-end pipeline tests pass
- [ ] Performance benchmarks meet targets
- [ ] Code coverage is >80%
- [ ] No linting errors
- [ ] Type checking passes
- [ ] Memory usage is reasonable

## ü§ù Contributing Tests

### Adding New Tests

1. **Unit Tests**: Add to `tests/unit/` for testing individual functions
2. **Integration Tests**: Add to `tests/integration/` for testing component interactions
3. **E2E Tests**: Add to `tests/end_to_end/` for testing complete workflows
4. **Performance Tests**: Add to `tests/performance/` for benchmarking

### Test Guidelines

- Use descriptive test names that explain what is being tested
- Include both positive and negative test cases
- Mock external dependencies appropriately
- Use fixtures for common test data
- Add performance assertions for critical paths
- Include edge cases and boundary conditions

### Mock Strategy

- **External APIs**: Always mock with realistic response data
- **Database**: Mock sessions and queries in unit tests
- **File System**: Use temporary directories
- **Time**: Mock datetime for consistent testing
- **Random Data**: Use fixed seeds for reproducibility

## üìù Test Reports

Test execution generates several reports:

- **HTML Reports**: `tests/results/` - Human-readable test results
- **Coverage Reports**: `tests/coverage/` - Code coverage analysis
- **JUnit XML**: `tests/results/junit.xml` - CI/CD integration
- **Benchmark JSON**: `tests/results/benchmark_results.json` - Performance data

## üö® Continuous Integration

The test suite is designed for CI/CD integration:

```yaml
# Example GitHub Actions workflow
- name: Run Tests
  run: |
    python scripts/run_tests.py --mode all --no-coverage
    
- name: Generate Coverage
  run: |
    python scripts/run_tests.py --mode coverage
    
- name: Upload Coverage
  uses: codecov/codecov-action@v3
  with:
    file: tests/coverage/coverage.xml
```

## üìö Additional Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [unittest.mock Documentation](https://docs.python.org/3/library/unittest.mock.html)
- [Coverage.py Documentation](https://coverage.readthedocs.io/)
- [Performance Testing Best Practices](https://pytest-benchmark.readthedocs.io/)

---

For questions about the test suite, please refer to the project documentation or contact the development team.